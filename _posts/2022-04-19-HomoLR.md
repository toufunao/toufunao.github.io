---
layout:     post   				    # 使用的布局（不需要改）
title:      HomoLR				# 标题 
subtitle:    #副标题
date:       2022-04-19 				# 时间
author:     Chris 						# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Federated Learning
    - Privacy Computing
---

## 1.Privacy-preserving entity resolution

1.两个数据方将数据使用CLK技术加密，将加密后的数据发给第三方

2.第三方收到数据方的加密数据后，使用Dice相关性系数计算相似性，并将其匹配

3.第三方分别将$\sigma,m$，$\tau，m$发回给双方。$\sigma,\tau$分别表示数据方如何调整数据的顺序，m代表两方是否存在相似的数据

## 2.泰勒公式近似的逻辑回归

训练数据是$(x_i,y_i)$

Logistic Loss Function

$l_S(\theta)=\frac{1}{n}\sum_{i_\in{s}}log(1+e^{-y_i\theta^Tx_i})$

Mini-batch SGD

$\nabla l_{S'}(\theta)=\frac{1}{S'}\sum_{i\in S'}(\frac{1}{1+e^{-y\theta^Tx}}-1)y_ix_i$



使用基于$log(1+e^{-z})$的泰勒展开

$log(1+e^{-z})=log2-\frac12z+\frac18z^2-\frac{1}{192}z^4+O(z^6)$

则在hold-out H上的损失函数为

$l_H(\theta)\approx\frac{1}{h}\sum_{i\in H}log2-\frac{1}{2}y_i\theta^Tx_i+\frac{1}{8}(\theta^Tx_i)^2$

梯度为

$\nabla l_{S'}(\theta)=\frac{1}{S'}\sum_{i\in S'}(\frac{1}{4}\theta^Tx_i-\frac{1}{2}y_i)x_i$



使用$[[m_i]]$进行加密后

$[[l_H(\theta)]]\approx[[\nu]]-\frac{1}{2}\theta^T[[\mu]]+\frac{1}{8h}[[m_i]](\theta^Tx_i)^2$

其中$[[\nu]]=\frac{1og_2}{h}\sum_{i\in H}[[m_i]]，\mu=\frac{1}{h}\sum_{i\in H}[[m_i]]y_ix_i$。因为$\mu$与模型$\theta$无关，论文在模型初始化时将其cache了

$[[\nabla l_{S'}(\theta)]]=\frac{1}{S'}\sum_{i\in S'}[[m_i]](\frac{1}{4}\theta^Tx_i-\frac{1}{2}y_i)x_i$

### 3. secure gradient流程

1.C->A 模型$\theta，batch size$ s'

2.A计算，发送模型$\theta$，当前batch $S'$和$残差([[u']])$ 给B

3.B计算本地梯度$[[Z]]$和$残差和(W)$，将$[[Z]]$和的$[[W]]$发送给A

4.A利用$[[W]]$计算自己的梯度$[[Z']]$，将$[[Z]],[[Z']]$一起发送给C

5.C根据$[[Z]]，[[Z]]'$计算全局梯度，并根据私钥进行解码

